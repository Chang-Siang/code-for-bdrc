{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# find consecutive missing values > 2 hours\n",
    "def find_missing_days(raw_data, column, counter_limit=1, trigger = 'DATE'):\n",
    "    \"\"\" Find consecutive missing values in column `feature` of  `data_raw`\n",
    "        consecutive missing values during the day.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        raw_data: dataframe, default=None\n",
    "            Must have column \"DATE\". \n",
    "\n",
    "        column: list, default=None\n",
    "            The column you want to search.\n",
    "        \n",
    "        counter_limit: int, default=1\n",
    "            The number of consecutive occurrences of the missing value you allow.\n",
    "            \n",
    "        trigger: string, default='DATE'\n",
    "            The trigger your used to reset counter.\n",
    "            must be datetime family [year, date, month, day, hour...]\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        missing_days: list\n",
    "            The list of missing days.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization\n",
    "    data = raw_data.copy()\n",
    "    missing_days = []\n",
    "    time_tag = pd.to_datetime('')\n",
    "    counter = 0\n",
    "\n",
    "    # traverse the data.\n",
    "    for i in range(len(data)):\n",
    "        row = data.iloc[i]\n",
    "        \n",
    "        # reset the trigger when enter a new [trigger]\n",
    "        if(time_tag != row[trigger]):\n",
    "            counter, time_tag = 0, row[trigger]\n",
    "            \n",
    "        # judgment status\n",
    "        if((counter>=counter_limit) & np.isnan(row[column])):\n",
    "            # append the day with continuous missing value\n",
    "            missing_days.append(row[trigger])\n",
    "        elif(np.isnan(row[column])):\n",
    "            # add missing value counter\n",
    "            counter += 1\n",
    "        else:\n",
    "            # compliance value, clear counter\n",
    "            counter = 0\n",
    "            \n",
    "    return np.unique(missing_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替函式 find_missing_days 新增了可以容許的連續缺失值數量 counter_limit\n",
    "# 考量到目前使用資料集的缺失值過多，建議將 counter_limit 設為 2\n",
    "missing_days = find_missing_days(merge_raw, column, counter_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     5,
     16
    ]
   },
   "outputs": [],
   "source": [
    "# 負責打包訓練和測試資料，僅用於歷遍資料，必須額外提供輸入和輸出變數組合\n",
    "def build_by_3day(_data_1h_raw, _data_1d_raw, _test_start_day, **args):\n",
    "    \"\"\" Build train/test dataset, only used to traverse the data. \n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        data_1h_raw: dataframe, default=None\n",
    "        \n",
    "        data_1d_raw: dataframe, default=None\n",
    "        \n",
    "        test_start_day: string, default='1996-01-01'\n",
    "        \n",
    "        **args: dict\n",
    "            ['set_input', 'set_output', 'set_idx']\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        train_x/test_x: list\n",
    "            The list of input data.\n",
    "            \n",
    "        train_y/test_y: list\n",
    "            The list of output data.\n",
    "\n",
    "        train_idx/test_idx: dataframe\n",
    "            The dataframe of data index.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization\n",
    "    _train_x, _train_y, _train_idx = [], [], []\n",
    "    _test_x, _test_y, _test_idx = [], [], []\n",
    "    _data_1h, _data_1d = _data_1h_raw.copy(), _data_1d_raw.copy()\n",
    "    _test_start_day = pd.to_datetime(_test_start_day).date()\n",
    "\n",
    "    # get 1day list, used to traverse the data set\n",
    "    _list_1d = _data_1h['TIME_TO_INTERVAL'].apply(lambda date: get_date_list(date))\n",
    "#     _list_1d = _list_1d.drop_duplicates().reset_index(inplace=False, drop=True)\n",
    "    \n",
    "    _list_1d = pd.date_range(start=_list_1d.min(), end=_list_1d.max())\n",
    "    _list_1d = _list_1d.to_series().apply(lambda x: x.date())\n",
    "    _list_1d = _list_1d.reset_index(inplace=False, drop=True)\n",
    "    \n",
    "    # traverse the data set\n",
    "    for i in range(len(_list_1d)-1):\n",
    "\n",
    "        # check whether the data in the specified range is missing in hours\n",
    "        _data_range = _list_1d[i-3:i+3]\n",
    "        _data_in_range_mask = _data_1h['TIME_TO_INTERVAL'].dt.date.isin(_data_range)\n",
    "        _data_in_range = _data_1h[_data_in_range_mask]\n",
    "\n",
    "        # 24 hour * 3 day * 2 (historical+future) = 144 point\n",
    "        # if there are missing values, skip this day\n",
    "        if (_data_in_range.isnull().values.any() or len(_data_in_range) != 144):\n",
    "            continue\n",
    "            \n",
    "        # if there is no missing value, then pack the data\n",
    "        _few_days, _target_days = _list_1d[i-3:i], _list_1d[i:i+3]\n",
    "        _few_day_by_1d = _data_1d[_data_1d['DATE'].isin(_few_days)]\n",
    "        _few_day_by_1h = _data_1h[_data_1h['DATE'].isin(_few_days)]\n",
    "        _target_day_by_1d = _data_1d[_data_1d['DATE'].isin(_target_days)]\n",
    "        _target_day_by_1h = _data_1h[_data_1h['DATE'].isin(_target_days)]\n",
    "        \n",
    "        # set your input & output\n",
    "        _input = args['_set_input'](\n",
    "            _few_day_by_1d, _few_day_by_1h, _target_day_by_1d, _target_day_by_1h)\n",
    "        \n",
    "        _output = args['_set_output'](\n",
    "            _few_day_by_1d, _few_day_by_1h, _target_day_by_1d, _target_day_by_1h)\n",
    "        \n",
    "        if \"_set_idx\" in args:\n",
    "            _idx = args['_set_idx'](\n",
    "                _few_day_by_1d, _few_day_by_1h, _target_day_by_1d, _target_day_by_1h)\n",
    "        else:\n",
    "            _idx = ''\n",
    "\n",
    "        # data for train or test\n",
    "        _isin_last_week = True\n",
    "        for date in _list_1d[i:i+3]:\n",
    "            _isin_last_week = _isin_last_week & isin_last_week(date, 7)\n",
    "        \n",
    "        _isin_test_data = _isin_last_week & (_list_1d[i+1] > _test_start_day)\n",
    "        \n",
    "        if(_isin_test_data):\n",
    "            _test_x.append(_input)\n",
    "            _test_y.append(_output)\n",
    "            _test_idx.append(_idx)\n",
    "        else:\n",
    "            _train_x.append(_input)\n",
    "            _train_y.append(_output)\n",
    "            _train_idx.append(_idx)\n",
    "            \n",
    "    _train_x, _train_y = np.array(_train_x), np.array(_train_y)\n",
    "    _test_x, _test_y = np.array(_test_x), np.array(_test_y)\n",
    "    _train_idx, _test_idx = pd.DataFrame(_train_idx), pd.DataFrame(_test_idx)\n",
    "    \n",
    "    return _train_x, _train_y, _train_idx, _test_x, _test_y, _test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要的調整項目在以下段落，過去直接根據資料產生日期清單的方式會有問題\n",
    "# 故調整為根據 date_range 產生日期清單的方式\n",
    "_list_1d = _data_1h['TIME_TO_INTERVAL'].apply(lambda date: get_date_list(date))\n",
    "_list_1d = pd.date_range(start=_list_1d.min(), end=_list_1d.max())\n",
    "_list_1d = _list_1d.to_series().apply(lambda x: x.date())\n",
    "_list_1d = _list_1d.reset_index(inplace=False, drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
